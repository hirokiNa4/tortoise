{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Autoencoders\n",
    "by [Mehdi Mirza](http://www-etud.iro.umontreal.ca/~mirzamom/)\n",
    "\n",
    "## Introduction\n",
    "This notebook will show you how to perform layer-wise pre-training using denoising autoencoders (DAEs), and subsequently stack the layers to form a multilayer perceptron (MLP) which can be fine-tuned using supervised training. You can also look at this [more detailed tutorial of training DAEs using Theano](http://deeplearning.net/tutorial/dA.html#daa) as well as [this tutorial](http://deeplearning.net/tutorial/SdA.html#sda) which covers the stacked version.\n",
    "\n",
    "The methods used here can easily be adapted to other models such as contractive auto-encoders (CAEs) or restricted Boltzmann machines (RBMs) with only small modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First layer\n",
    "\n",
    "The first layer and its training algorithm are defined in the file `dae_l1.yaml`. Here we load the model and set some of its hypyerparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!obj:pylearn2.train.Train {\n",
      "    dataset: &train !obj:pylearn2.datasets.mnist.MNIST {\n",
      "        which_set: 'train',\n",
      "        start: 0,\n",
      "        stop: 50000\n",
      "    },\n",
      "    model: !obj:pylearn2.models.autoencoder.DenoisingAutoencoder {\n",
      "        nvis : 784,\n",
      "        nhid : 500,\n",
      "        irange : 0.05,\n",
      "        corruptor: !obj:pylearn2.corruption.BinomialCorruptor {\n",
      "            corruption_level: .2,\n",
      "        },\n",
      "        act_enc: \"tanh\",\n",
      "        act_dec: null,    # Linear activation on the decoder side.\n",
      "    },\n",
      "    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {\n",
      "        learning_rate : 1e-3,\n",
      "        batch_size : 100,\n",
      "        monitoring_batches : 5,\n",
      "        monitoring_dataset : *train,\n",
      "        cost : !obj:pylearn2.costs.autoencoder.MeanSquaredReconstructionError {},\n",
      "        termination_criterion : !obj:pylearn2.termination_criteria.EpochCounter {\n",
      "            max_epochs: 10,\n",
      "        },\n",
      "    },\n",
      "    save_path: \"./dae_l1.pkl\",\n",
      "    save_freq: 1\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer1_yaml = open('dae_l1.yaml', 'r').read()\n",
    "hyper_params_l1 = {'train_stop' : 50000,\n",
    "                   'batch_size' : 100,\n",
    "                   'monitoring_batches' : 5,\n",
    "                   'nhid' : 500,\n",
    "                   'max_epochs' : 10,\n",
    "                   'save_path' : '.'}\n",
    "layer1_yaml = layer1_yaml % (hyper_params_l1)\n",
    "print layer1_yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model using the YAML string in the same way as the previous tutorials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter and initial learning rate summary:\n",
      "\tvb: 0.0010000000475\n",
      "\thb: 0.0010000000475\n",
      "\tW: 0.0010000000475\n",
      "\tWprime: 0.0010000000475\n",
      "Compiling sgd_update...\n",
      "Compiling sgd_update done. Time elapsed: 0.000000 seconds\n",
      "compiling begin_record_entry...\n",
      "compiling begin_record_entry done. Time elapsed: 0.000000 seconds\n",
      "Monitored channels: \n",
      "\tlearning_rate\n",
      "\tmonitor_seconds_per_epoch\n",
      "\tobjective\n",
      "Compiling accum...\n",
      "graph size: 23\n",
      "Compiling accum done. Time elapsed: 0.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 0\n",
      "\tBatches seen: 0\n",
      "\tExamples seen: 0\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 0.0\n",
      "\tobjective: 85.4375915527\n",
      "Time this epoch: 1.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 1\n",
      "\tBatches seen: 500\n",
      "\tExamples seen: 50000\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 1.0\n",
      "\tobjective: 29.1613636017\n",
      "Saving to ./dae_l1.pkl...\n",
      "Saving to ./dae_l1.pkl done. Time elapsed: 0.000000 seconds\n",
      "Time this epoch: 1.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 2\n",
      "\tBatches seen: 1000\n",
      "\tExamples seen: 100000\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 1.0\n",
      "\tobjective: 21.9736881256\n",
      "Saving to ./dae_l1.pkl...\n",
      "Saving to ./dae_l1.pkl done. Time elapsed: 0.000000 seconds\n",
      "Time this epoch: 1.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 3\n",
      "\tBatches seen: 1500\n",
      "\tExamples seen: 150000\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 1.0\n",
      "\tobjective: 18.4479560852\n",
      "Saving to ./dae_l1.pkl...\n",
      "Saving to ./dae_l1.pkl done. Time elapsed: 0.000000 seconds\n",
      "Time this epoch: 1.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 4\n",
      "\tBatches seen: 2000\n",
      "\tExamples seen: 200000\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 1.0\n",
      "\tobjective: 16.2897148132\n",
      "Saving to ./dae_l1.pkl...\n",
      "Saving to ./dae_l1.pkl done. Time elapsed: 0.000000 seconds\n",
      "Time this epoch: 1.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 5\n",
      "\tBatches seen: 2500\n",
      "\tExamples seen: 250000\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 1.0\n",
      "\tobjective: 14.8111886978\n",
      "Saving to ./dae_l1.pkl...\n",
      "Saving to ./dae_l1.pkl done. Time elapsed: 0.000000 seconds\n",
      "Time this epoch: 1.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 6\n",
      "\tBatches seen: 3000\n",
      "\tExamples seen: 300000\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 1.0\n",
      "\tobjective: 13.6504278183\n",
      "Saving to ./dae_l1.pkl...\n",
      "Saving to ./dae_l1.pkl done. Time elapsed: 0.000000 seconds\n",
      "Time this epoch: 1.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 7\n",
      "\tBatches seen: 3500\n",
      "\tExamples seen: 350000\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 1.0\n",
      "\tobjective: 12.9274587631\n",
      "Saving to ./dae_l1.pkl...\n",
      "Saving to ./dae_l1.pkl done. Time elapsed: 0.000000 seconds\n",
      "Time this epoch: 1.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 8\n",
      "\tBatches seen: 4000\n",
      "\tExamples seen: 400000\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 1.0\n",
      "\tobjective: 12.2765922546\n",
      "Saving to ./dae_l1.pkl...\n",
      "Saving to ./dae_l1.pkl done. Time elapsed: 0.000000 seconds\n",
      "Time this epoch: 1.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 9\n",
      "\tBatches seen: 4500\n",
      "\tExamples seen: 450000\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 1.0\n",
      "\tobjective: 11.7446937561\n",
      "Saving to ./dae_l1.pkl...\n",
      "Saving to ./dae_l1.pkl done. Time elapsed: 0.000000 seconds\n",
      "Time this epoch: 1.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 10\n",
      "\tBatches seen: 5000\n",
      "\tExamples seen: 500000\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 1.0\n",
      "\tobjective: 11.4141273499\n",
      "Saving to ./dae_l1.pkl...\n",
      "Saving to ./dae_l1.pkl done. Time elapsed: 0.000000 seconds\n",
      "Saving to ./dae_l1.pkl...\n",
      "Saving to ./dae_l1.pkl done. Time elapsed: 1.000000 seconds\n"
     ]
    }
   ],
   "source": [
    "from pylearn2.config import yaml_parse\n",
    "train = yaml_parse.load(layer1_yaml)\n",
    "train.main_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second layer\n",
    "The second layer takes the output of the first layer as its input. Hence we must first apply the first layer's transformations to the raw data using `datasets.transformer_dataset.TransformerDataset`. This class takes two arguments:\n",
    "\n",
    "   - `raw`: the raw data\n",
    "   - `transformer`: a Pylearn2 block that transforms the raw data, which in our case is the `dae_l1.pkl` file from the previous step\n",
    "\n",
    "To train the second layer, we load the YAML file as before and set the hyperparameters before starting the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!obj:pylearn2.train.Train {\n",
      "    dataset: &train !obj:pylearn2.datasets.transformer_dataset.TransformerDataset {\n",
      "        raw: !obj:pylearn2.datasets.mnist.MNIST {\n",
      "            which_set: 'train',\n",
      "            start: 0,\n",
      "            stop: 50000\n",
      "        },\n",
      "        transformer: !pkl: \"./dae_l1.pkl\"\n",
      "    },\n",
      "    model: !obj:pylearn2.models.autoencoder.DenoisingAutoencoder {\n",
      "        nvis : 500,\n",
      "        nhid : 500,\n",
      "        irange : 0.05,\n",
      "        corruptor: !obj:pylearn2.corruption.BinomialCorruptor {\n",
      "            corruption_level: .3,\n",
      "        },\n",
      "        act_enc: \"tanh\",\n",
      "        act_dec: null,    # Linear activation on the decoder side.\n",
      "    },\n",
      "    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {\n",
      "        learning_rate : 1e-3,\n",
      "        batch_size : 100,\n",
      "        monitoring_batches : 5,\n",
      "        monitoring_dataset : *train,\n",
      "        cost : !obj:pylearn2.costs.autoencoder.MeanSquaredReconstructionError {},\n",
      "        termination_criterion : !obj:pylearn2.termination_criteria.EpochCounter {\n",
      "            max_epochs: 10,\n",
      "        },\n",
      "    },\n",
      "    save_path: \"./dae_l2.pkl\",\n",
      "    save_freq: 1\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer2_yaml = open('dae_l2.yaml', 'r').read()\n",
    "hyper_params_l2 = {'train_stop' : 50000,\n",
    "                   'batch_size' : 100,\n",
    "                   'monitoring_batches' : 5,\n",
    "                   'nvis' : hyper_params_l1['nhid'],\n",
    "                   'nhid' : 500,\n",
    "                   'max_epochs' : 10,\n",
    "                   'save_path' : '.'}\n",
    "layer2_yaml = layer2_yaml % (hyper_params_l2)\n",
    "print layer2_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter and initial learning rate summary:\n",
      "\tvb: 0.0010000000475\n",
      "\thb: 0.0010000000475\n",
      "\tW: 0.0010000000475\n",
      "\tWprime: 0.0010000000475\n",
      "Compiling sgd_update...\n",
      "Compiling sgd_update done. Time elapsed: 0.000000 seconds\n",
      "compiling begin_record_entry...\n",
      "compiling begin_record_entry done. Time elapsed: 0.000000 seconds\n",
      "Monitored channels: \n",
      "\tlearning_rate\n",
      "\tmonitor_seconds_per_epoch\n",
      "\tobjective\n",
      "Compiling accum...\n",
      "graph size: 23\n",
      "Compiling accum done. Time elapsed: 0.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 0\n",
      "\tBatches seen: 0\n",
      "\tExamples seen: 0\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 0.0\n",
      "\tobjective: 51.0506210327\n",
      "Time this epoch: 1.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 1\n",
      "\tBatches seen: 500\n",
      "\tExamples seen: 50000\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 1.0\n",
      "\tobjective: 20.0142116547\n",
      "Saving to ./dae_l2.pkl...\n",
      "Saving to ./dae_l2.pkl done. Time elapsed: 0.000000 seconds\n",
      "Time this epoch: 1.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 2\n",
      "\tBatches seen: 1000\n",
      "\tExamples seen: 100000\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 1.0\n",
      "\tobjective: 12.8833475113\n",
      "Saving to ./dae_l2.pkl...\n",
      "Saving to ./dae_l2.pkl done. Time elapsed: 0.000000 seconds\n",
      "Time this epoch: 1.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 3\n",
      "\tBatches seen: 1500\n",
      "\tExamples seen: 150000\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 1.0\n",
      "\tobjective: 9.65194129944\n",
      "Saving to ./dae_l2.pkl...\n",
      "Saving to ./dae_l2.pkl done. Time elapsed: 0.000000 seconds\n",
      "Time this epoch: 1.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 4\n",
      "\tBatches seen: 2000\n",
      "\tExamples seen: 200000\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 1.0\n",
      "\tobjective: 7.71482992172\n",
      "Saving to ./dae_l2.pkl...\n",
      "Saving to ./dae_l2.pkl done. Time elapsed: 0.000000 seconds\n",
      "Time this epoch: 1.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 5\n",
      "\tBatches seen: 2500\n",
      "\tExamples seen: 250000\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 1.0\n",
      "\tobjective: 6.5238275528\n",
      "Saving to ./dae_l2.pkl...\n",
      "Saving to ./dae_l2.pkl done. Time elapsed: 0.000000 seconds\n",
      "Time this epoch: 1.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 6\n",
      "\tBatches seen: 3000\n",
      "\tExamples seen: 300000\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 1.0\n",
      "\tobjective: 5.69179153442\n",
      "Saving to ./dae_l2.pkl...\n",
      "Saving to ./dae_l2.pkl done. Time elapsed: 0.000000 seconds\n",
      "Time this epoch: 1.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 7\n",
      "\tBatches seen: 3500\n",
      "\tExamples seen: 350000\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 1.0\n",
      "\tobjective: 5.15888118744\n",
      "Saving to ./dae_l2.pkl...\n",
      "Saving to ./dae_l2.pkl done. Time elapsed: 0.000000 seconds\n",
      "Time this epoch: 1.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 8\n",
      "\tBatches seen: 4000\n",
      "\tExamples seen: 400000\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 1.0\n",
      "\tobjective: 4.75159025192\n",
      "Saving to ./dae_l2.pkl...\n",
      "Saving to ./dae_l2.pkl done. Time elapsed: 0.000000 seconds\n",
      "Time this epoch: 1.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 9\n",
      "\tBatches seen: 4500\n",
      "\tExamples seen: 450000\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 1.0\n",
      "\tobjective: 4.38682460785\n",
      "Saving to ./dae_l2.pkl...\n",
      "Saving to ./dae_l2.pkl done. Time elapsed: 0.000000 seconds\n",
      "Time this epoch: 1.000000 seconds\n",
      "Monitoring step:\n",
      "\tEpochs seen: 10\n",
      "\tBatches seen: 5000\n",
      "\tExamples seen: 500000\n",
      "\tlearning_rate: 0.00100000016391\n",
      "\tmonitor_seconds_per_epoch: 1.0\n",
      "\tobjective: 4.21171569824\n",
      "Saving to ./dae_l2.pkl...\n",
      "Saving to ./dae_l2.pkl done. Time elapsed: 0.000000 seconds\n",
      "Saving to ./dae_l2.pkl...\n",
      "Saving to ./dae_l2.pkl done. Time elapsed: 0.000000 seconds\n"
     ]
    }
   ],
   "source": [
    "train = yaml_parse.load(layer2_yaml)\n",
    "train.main_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised fine-tuning\n",
    "Now that we have two pre-trained layers, we can stack them to form an MLP which can be trained in a supervised fashion. We use the MLP class as usual for this, except that we now use `models.mlp.PretrainedLayer` for the different layers so that we can pass our pre-trained layers (as pickle files) using the `layer_content` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!obj:pylearn2.train.Train {\n",
      "    dataset: &train !obj:pylearn2.datasets.mnist.MNIST {\n",
      "        which_set: 'train',\n",
      "        start: 0,\n",
      "        stop: 50000\n",
      "    },\n",
      "    model: !obj:pylearn2.models.mlp.MLP {\n",
      "        batch_size: 100,\n",
      "        layers: [\n",
      "                 !obj:pylearn2.models.mlp.PretrainedLayer {\n",
      "                     layer_name: 'h1',\n",
      "                     layer_content: !pkl: \"./dae_l1.pkl\"\n",
      "                 },\n",
      "                 !obj:pylearn2.models.mlp.PretrainedLayer {\n",
      "                     layer_name: 'h2',\n",
      "                     layer_content: !pkl: \"./dae_l2.pkl\"\n",
      "                 },\n",
      "                 !obj:pylearn2.models.mlp.Softmax {\n",
      "                     max_col_norm: 1.9365,\n",
      "                     layer_name: 'y',\n",
      "                     n_classes: 10,\n",
      "                     irange: .005\n",
      "                 }\n",
      "                ],\n",
      "        nvis: 784\n",
      "    },\n",
      "    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {\n",
      "        learning_rate: .05,\n",
      "        learning_rule: !obj:pylearn2.training_algorithms.learning_rule.Momentum {\n",
      "            init_momentum: .5,\n",
      "        },\n",
      "        monitoring_dataset:\n",
      "            {\n",
      "                'valid' : !obj:pylearn2.datasets.mnist.MNIST {\n",
      "                              which_set: 'train',\n",
      "                              start: 50000,\n",
      "                              stop: 60000\n",
      "                          },\n",
      "            },\n",
      "        cost: !obj:pylearn2.costs.mlp.Default {},\n",
      "        termination_criterion: !obj:pylearn2.termination_criteria.And {\n",
      "            criteria: [\n",
      "                !obj:pylearn2.termination_criteria.MonitorBased {\n",
      "                    channel_name: \"valid_y_misclass\",\n",
      "                    prop_decrease: 0.,\n",
      "                    N: 100\n",
      "                },\n",
      "                !obj:pylearn2.termination_criteria.EpochCounter {\n",
      "                    max_epochs: 50\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        update_callbacks: !obj:pylearn2.training_algorithms.sgd.ExponentialDecay {\n",
      "            decay_factor: 1.00004,\n",
      "            min_lr: .000001\n",
      "        }\n",
      "    },\n",
      "    extensions: [\n",
      "        !obj:pylearn2.training_algorithms.learning_rule.MomentumAdjustor {\n",
      "            start: 1,\n",
      "            saturate: 250,\n",
      "            final_momentum: .7\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp_yaml = open('dae_mlp.yaml', 'r').read()\n",
    "hyper_params_mlp = {'train_stop' : 50000,\n",
    "                    'valid_stop' : 60000,\n",
    "                    'batch_size' : 100,\n",
    "                    'max_epochs' : 50,\n",
    "                    'save_path' : '.'}\n",
    "mlp_yaml = mlp_yaml % (hyper_params_mlp)\n",
    "print mlp_yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "train = yaml_parse.load(mlp_yaml)\n",
    "train.main_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
